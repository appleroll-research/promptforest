{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848630a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/28zhany/promptforest/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.calibration import calibration_curve\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45beb85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTROL_MODELS = [\n",
    "    {\n",
    "        \"name\": \"ProtectAI Deberta V3\",\n",
    "        \"model_name\": \"protectai/deberta-v3-base-prompt-injection-v2\",\n",
    "        \"device\": \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Deepset\",\n",
    "        \"model_name\": \"deepset/deberta-v3-base-injection\",\n",
    "        \"device\": \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Vijil Dome\",\n",
    "        \"model_name\": \"vijil/vijil_dome_prompt_injection_detection\",\n",
    "        \"device\": \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Qualifire\",\n",
    "        \"model_name\": \"qualifire/prompt-injection-sentinel\",\n",
    "        \"device\": \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Qualifire v2\",\n",
    "        \"model_name\": \"qualifire/prompt-injection-jailbreak-sentinel-v2\",\n",
    "        \"device\": \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"name\": \"Llama PromptGuard 86M\",\n",
    "        \"model_name\": \"meta-llama/Llama-Prompt-Guard-2-86M\",\n",
    "        \"device\": \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    },\n",
    "\n",
    "\n",
    "]\n",
    "PF_SERVER_URL = \"http://localhost:1000/analyze\"\n",
    "MAX_SAMPLES = 3000\n",
    "BENCHMARK_DATASET = \"qualifire/prompt-injections-benchmark\"\n",
    "BENCHMARK_SPLIT = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6b6a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_label(value):\n",
    "    if isinstance(value, int): return value\n",
    "    if isinstance(value, str):\n",
    "        v = value.lower()\n",
    "        if v in [\"jailbreak\", \"malicious\", \"unsafe\", \"attack\", \"injection\", \"1\"]: return 1\n",
    "        if v in [\"benign\", \"safe\", \"legit\", \"0\"]: return 0\n",
    "    try: return int(value)\n",
    "    except: return 0\n",
    "\n",
    "def get_control_prediction(text, tokenizer, model, device):\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        pred = torch.argmax(probs, dim=-1).item()\n",
    "        conf = probs[0][1].item()  # probability of class 1 (malicious)\n",
    "        return pred, conf\n",
    "    except Exception as e:\n",
    "        print(f\"Error inferencing: {e}\")\n",
    "        return 0, 0.5\n",
    "\n",
    "def get_pf_prediction(text):\n",
    "    try:\n",
    "        resp = requests.post(PF_SERVER_URL, json={\"prompt\": text}, timeout=5)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        pred_label = int(data.get(\"is_malicious\", 0))\n",
    "        conf = data.get(\"confidence\")\n",
    "        if conf is None:\n",
    "            prob_malicious = 1.0 if pred_label == 1 else 0.0\n",
    "        else:\n",
    "            prob_malicious = conf if pred_label == 1 else 1 - conf\n",
    "        return pred_label, prob_malicious\n",
    "    except Exception as e:\n",
    "        print(f\"Request failed for text: {text[:30]}... -> {e}\")\n",
    "        return 0, 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5178734",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(BENCHMARK_DATASET, split=BENCHMARK_SPLIT)\n",
    "ds = ds.shuffle(seed=42).select(range(min(len(ds), MAX_SAMPLES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c678b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading control model: protectai/deberta-v3-base-prompt-injection-v2 on mps...\n",
      "Running inference for ProtectAI Deberta V3 (HuggingFace model)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProtectAI Deberta V3: 100%|██████████| 3000/3000 [01:17<00:00, 38.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading control model: deepset/deberta-v3-base-injection on mps...\n",
      "Running inference for Deepset (HuggingFace model)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deepset: 100%|██████████| 3000/3000 [01:13<00:00, 40.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading control model: vijil/vijil_dome_prompt_injection_detection on mps...\n",
      "Running inference for Vijil Dome (HuggingFace model)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vijil Dome: 100%|██████████| 3000/3000 [00:57<00:00, 52.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading control model: qualifire/prompt-injection-sentinel on mps...\n",
      "Running inference for Qualifire (HuggingFace model)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Qualifire: 100%|██████████| 3000/3000 [01:44<00:00, 28.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading control model: qualifire/prompt-injection-jailbreak-sentinel-v2 on mps...\n",
      "Running inference for Qualifire v2 (HuggingFace model)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Qualifire v2: 100%|██████████| 3000/3000 [02:55<00:00, 17.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading control model: meta-llama/Llama-Prompt-Guard-2-86M on mps...\n",
      "Running inference for Llama PromptGuard 86M (HuggingFace model)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama PromptGuard 86M: 100%|██████████| 3000/3000 [01:23<00:00, 35.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference for PromptForest (API)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PromptForest: 100%|██████████| 3000/3000 [03:02<00:00, 16.45it/s]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# Run inference for HuggingFace control models\n",
    "for control in CONTROL_MODELS:\n",
    "    model_name = control['model_name']\n",
    "    device = control['device']\n",
    "    print(f\"Loading control model: {model_name} on {device}...\")\n",
    "    if model_name.startswith(\"vijil\"):\n",
    "        # Use modernbert tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/modernBERT-base\")\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"Running inference for {control['name']} (HuggingFace model)...\")\n",
    "    for ex in tqdm(ds, desc=control['name']):\n",
    "        text = ex.get(\"text\") or ex.get(\"prompt\")\n",
    "        if not text:\n",
    "            continue\n",
    "        label = parse_label(ex[\"label\"])\n",
    "        pred_label, prob_malicious = get_control_prediction(text, tokenizer, model, device)\n",
    "        confidence = prob_malicious if pred_label == 1 else 1 - prob_malicious\n",
    "        results.append({\n",
    "            \"model\": control['name'],\n",
    "            \"label\": label,\n",
    "            \"prediction\": pred_label,\n",
    "            \"prob_malicious\": prob_malicious,\n",
    "            \"confidence\": confidence,\n",
    "            \"is_correct\": pred_label == label\n",
    "        })\n",
    "\n",
    "# Run inference for PromptForest API\n",
    "print(\"Running inference for PromptForest (API)...\")\n",
    "for ex in tqdm(ds, desc=\"PromptForest\"):\n",
    "    text = ex.get(\"text\") or ex.get(\"prompt\")\n",
    "    if not text:\n",
    "        continue\n",
    "    label = parse_label(ex[\"label\"])\n",
    "    pred_label, prob_malicious = get_pf_prediction(text)\n",
    "    confidence = prob_malicious if pred_label == 1 else 1 - prob_malicious\n",
    "    results.append({\n",
    "        \"model\": \"PromptForest\",\n",
    "        \"label\": label,\n",
    "        \"prediction\": pred_label,\n",
    "        \"prob_malicious\": prob_malicious,\n",
    "        \"confidence\": confidence,\n",
    "        \"is_correct\": pred_label == label\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873b4b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "      <th>prob_malicious</th>\n",
       "      <th>confidence</th>\n",
       "      <th>is_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ProtectAI Deberta V3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.978874e-04</td>\n",
       "      <td>0.999802</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ProtectAI Deberta V3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9.941406e-01</td>\n",
       "      <td>0.994141</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ProtectAI Deberta V3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.940697e-07</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ProtectAI Deberta V3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.344650e-07</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ProtectAI Deberta V3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model  label  prediction  prob_malicious  confidence  \\\n",
       "0  ProtectAI Deberta V3      1           0    1.978874e-04    0.999802   \n",
       "1  ProtectAI Deberta V3      1           1    9.941406e-01    0.994141   \n",
       "2  ProtectAI Deberta V3      0           0    8.940697e-07    0.999999   \n",
       "3  ProtectAI Deberta V3      0           0    8.344650e-07    0.999999   \n",
       "4  ProtectAI Deberta V3      0           1    1.000000e+00    1.000000   \n",
       "\n",
       "   is_correct  \n",
       "0       False  \n",
       "1        True  \n",
       "2        True  \n",
       "3        True  \n",
       "4       False  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregate results\n",
    "df = pd.DataFrame(results)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d3d5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "metrics = defaultdict(dict)\n",
    "\n",
    "for model in df['model'].unique():\n",
    "    # Compute metrics for each model\n",
    "    sub = df[df['model'] == model]\n",
    "    accuracy = sub['is_correct'].mean()\n",
    "    wrong_df = sub[~sub['is_correct']]\n",
    "    right_df = sub[sub['is_correct']]\n",
    "    avg_conf_wrong = wrong_df['confidence'].mean() if not wrong_df.empty else 0\n",
    "    avg_conf_right = right_df['confidence'].mean() if not right_df.empty else 0\n",
    "    probs = np.array(sub['prob_malicious'])\n",
    "    labels = np.array(sub['label'])\n",
    "    prob_true, prob_pred = calibration_curve(labels, probs, n_bins=10)\n",
    "    ece = np.mean(np.abs(prob_true - prob_pred))\n",
    "    metrics[model]['accuracy'] = accuracy\n",
    "    metrics[model]['avg_conf_wrong'] = avg_conf_wrong\n",
    "    metrics[model]['ece'] = ece\n",
    "    metrics[model]['prob_true'] = prob_true\n",
    "    metrics[model]['prob_pred'] = prob_pred\n",
    "    metrics[model]['avg_conf_right'] = avg_conf_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89683cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Avg Conf (Wrong)</th>\n",
       "      <th>Avg Conf (Right)</th>\n",
       "      <th>ECE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ProtectAI Deberta V3</th>\n",
       "      <td>0.717667</td>\n",
       "      <td>0.977428</td>\n",
       "      <td>0.989692</td>\n",
       "      <td>0.224269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Deepset</th>\n",
       "      <td>0.595333</td>\n",
       "      <td>0.992377</td>\n",
       "      <td>0.995169</td>\n",
       "      <td>0.422659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vijil Dome</th>\n",
       "      <td>0.903333</td>\n",
       "      <td>0.923159</td>\n",
       "      <td>0.989999</td>\n",
       "      <td>0.213882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qualifire</th>\n",
       "      <td>0.981667</td>\n",
       "      <td>0.908524</td>\n",
       "      <td>0.998704</td>\n",
       "      <td>0.298766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qualifire v2</th>\n",
       "      <td>0.973000</td>\n",
       "      <td>0.763157</td>\n",
       "      <td>0.984574</td>\n",
       "      <td>0.096029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama PromptGuard 86M</th>\n",
       "      <td>0.762667</td>\n",
       "      <td>0.898650</td>\n",
       "      <td>0.969526</td>\n",
       "      <td>0.192880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PromptForest</th>\n",
       "      <td>0.901000</td>\n",
       "      <td>0.642387</td>\n",
       "      <td>0.836887</td>\n",
       "      <td>0.070384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Accuracy  Avg Conf (Wrong)  Avg Conf (Right)       ECE\n",
       "Model                                                                        \n",
       "ProtectAI Deberta V3   0.717667          0.977428          0.989692  0.224269\n",
       "Deepset                0.595333          0.992377          0.995169  0.422659\n",
       "Vijil Dome             0.903333          0.923159          0.989999  0.213882\n",
       "Qualifire              0.981667          0.908524          0.998704  0.298766\n",
       "Qualifire v2           0.973000          0.763157          0.984574  0.096029\n",
       "Llama PromptGuard 86M  0.762667          0.898650          0.969526  0.192880\n",
       "PromptForest           0.901000          0.642387          0.836887  0.070384"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Give a summary table\n",
    "summary = []\n",
    "for model in df['model'].unique():\n",
    "    summary.append({\n",
    "        'Model': model,\n",
    "        'Accuracy': metrics[model]['accuracy'],\n",
    "        'Avg Conf (Wrong)': metrics[model]['avg_conf_wrong'],\n",
    "        'Avg Conf (Right)': metrics[model]['avg_conf_right'],\n",
    "        'ECE': metrics[model]['ece'],\n",
    "    })\n",
    "pd.DataFrame(summary).set_index('Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f947c5ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m plt.figure(figsize=(\u001b[32m7\u001b[39m, \u001b[32m7\u001b[39m))\n\u001b[32m      6\u001b[39m colors = {\u001b[33m\"\u001b[39m\u001b[33mControl\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mblue\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPromptForest\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33morange\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m].unique():\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mPromptForest\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mQualifire v2\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mLlama PromptGuard 86M\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m     10\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 700x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.interpolate import make_interp_spline\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "colors = {\"Control\": \"blue\", \"PromptForest\": \"orange\"}\n",
    "\n",
    "for model in df['model'].unique():\n",
    "    if model not in ['PromptForest', 'Qualifire v2', 'Llama PromptGuard 86M']:\n",
    "        continue\n",
    "    color = colors.get(model, None)\n",
    "    \n",
    "    x = np.array(metrics[model]['prob_pred'])\n",
    "    y = np.array(metrics[model]['prob_true'])\n",
    "    \n",
    "    x_smooth = np.linspace(x.min(), x.max(), 300)\n",
    "    spline = make_interp_spline(x, y, k=3)  # cubic spline\n",
    "    y_smooth = spline(x_smooth)\n",
    "    \n",
    "    plt.plot(x_smooth, y_smooth, label=f\"{model} (ECE={metrics[model]['ece']:.3f})\", color=color)\n",
    "    plt.scatter(x, y, s=0, color=color) # remove s=0 for visible dots.\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.ylabel('Fraction of Positives')\n",
    "plt.title('Fraction of Positives vs. Mean Predicted Probability')\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
